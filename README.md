# Deep Learning from Scratch: Build Neural Network âœ¦â‹†ï½¡Ëš ð“†‰ ð“† ð“†¡ â‹†ï½¡Ëšâœ¦

Learn ML &amp; Deep Learning from scratch â€” build logistic regression, neural networks, and deep models from scratch with numpy.

This repository contains hands-on notebooks, implementations, and notes that walk through the foundations of **Machine Learning (ML)** and **Deep Learning (DL)** â€” starting from **Logistic Regression with a Neural Network** and gradually building up to **Deep Neural Networks (DNNs)**.  

The focus is on understanding concepts deeply by implementing them manually with `numpy`, before moving on to higher-level frameworks.

---

## ðŸŽ¯ Purpose

This project serves as a structured, practical learning path for:

- ðŸ“Œ Building ML/DL algorithms **from scratch** (without black-box frameworks)  
- ðŸ“Œ Developing strong intuition about **forward/backward propagation, cost functions, and optimization**  
- ðŸ“Œ Understanding how modern deep networks extend simple models like **Logistic Regression**  
- ðŸ“Œ Preparing a solid foundation for frameworks like **TensorFlow** and **PyTorch**  

---

## ðŸ“š Topics Covered

### 1ï¸âƒ£ Python & Numpy Basics for ML
- Vectorization vs loops  
- Sigmoid, Softmax, and their derivatives  
- Reshaping and normalizing data  
- Loss functions: L1, L2  

---

### 2ï¸âƒ£ Logistic Regression with a Neural Network
- Initializing parameters  
- Forward propagation  
- Cost function & gradients  
- Optimization with Gradient Descent  
- Building the Logistic Regression classifier for image recognition

---

### 3ï¸âƒ£ Building a Shallow Neural Network (2-layer)
- Adding a hidden layer  
- Using activation functions like **ReLU** and **tanh**  
- Implementing **backpropagation step by step**  
- Comparing results with Logistic Regression  

---

### 4ï¸âƒ£ Building Your Deep Neural Network (L-layers)
- Generalizing to **L-layer networks**  
- Forward Propagation: **Linear â†’ Activation â†’ Model**  
- Backward Propagation for deep nets  
- Parameter updates  
- Putting everything into a **`NeuralNetwork` class**  

---

### 5ï¸âƒ£ Optimization & Training Tricks
- Learning rate tuning  
- Initialization strategies (**Xavier, He**)  
- Gradient checking  
- Regularization: **L2, Dropout**  
- Mini-batch Gradient Descent  
- Advanced optimizers: **Adam**  

---


## ðŸ“š Recommended Learning Resources

- [Deep Learning Specialization (Coursera)](https://www.coursera.org/specializations/deep-learning)  
- [PyTorch Documentation](https://pytorch.org/docs/stable/)  
- [TensorFlow Documentation](https://www.tensorflow.org/)  

---

## ðŸ™Œ Acknowledgements

Special thanks to the open-source ML/DL community and course creators who inspired this learning journey.

> This repository is based on my personal learning experience.


